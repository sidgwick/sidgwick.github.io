---
title: "决策树算法"
date: 2023-09-20 21:14:41
tags: ai decision-tree
---

# 决策树算法原理

## 树模型

从**决策树**根节点开始一步步走到叶子节点(决策), 所有的数据最终都会落到叶子节点, 利用决策树既可以做分类也可以做回归.

**树的组成**

- **根节点**: 第一个选择点
- **非叶子节点与分支**: 中间过程
- **叶子节点**: 最终的决策结果

决策树的训练是从给定的训练集构造(从根节点开始选择特征, 如何进行特征切分)出来一棵树, 然后将测试数据根据构造出来的树模型从上到下去走一遍, 得到决策结果.

一旦构造好了决策树, 那么分类或者预测任务就很简单了, 只需跟着树走一遍决策流程就可以了, 那么难点就在于如何构造出来一颗树.

<!--more-->

## 熵的作用

根节点的选择该用哪个特征呢? 我们的目标应该是经过根节点的决策之后尽可能好的切分数据(即决策后分类的效果更好), 然后再找除了根节点用到的特征之外的其他特征中切分效果最好的特征作为决策树的后续节点, 然后是第三好, 第四好...

所以我们需要一种衡量标准, 来计算通过不同特征进行分支选择后的分类 情况, 找出来最好的那个当成根节点.

### 熵

熵是表示随机变量不确定性的度量, 它的定义如下:

$$
H(X) = -\sum_{i=1}^{n} p_{i}\log{p_{i}}
$$

举例有 $A = \lbrace 1,1,1,1,1,1,1,1,2,2 \rbrace$, $B=\lbrace 1,2,3,4,5,6,7,8,9,1 \rbrace$ 两个集合, 显然 $A$ 集合的熵值要低，因为 $A$ 里面只有两种类别, 相对稳定一些. 而 $B$ 中类别太多了, 熵值就会大很多. (在分类任务中我们希望通过节点分支后数据类别的熵值大还是小呢?)

## 信息增益原理

集合中的信息越混乱, 得到的熵值也就越大. 如当 $p=0$ 或 $p=1$ 时, $H(p)=0$, 随机变量完全没有不确定性. 当 $p=0.5$ 时, $H(p)=1$, 此时随机变量的不确定性最大.

### 信息增益

指的是在经过特征 $X$ 的分类之后, 使得结果熵相对于分类操作之前的熵的减小值.

```
四、决策树构造实例
1、实例


数据
14天打球情况

特征
4种环境变化

目标
构造决策树，判断当出现一种天气的情况下，打不打球。

2、决策树构造


划分方式：4种

问题：谁当根节点呢?

依据：信息增益

例子：基于天气划分


在历史数据中(14天)有9天打球，5天不打球，所以此时的熵应为:


4个特征逐一分析，先从outlook特征开始:
Outlook = sunny时，熵值为0.971

Outlook = overcast时，熵值为0

Outlook = rainy时，熵值为0.971

加权计算
根据数据统计，outlook取值分别为sunny,overcast,rainy的概率分别为：5/14, 4/14, 5/14

熵值计算：5/14 * 0.971 + 4/14 * 0 + 5/14 * 0.971 = 0.693

(gain(temperature)=0.029 gain(humidity)=0.152 gain(windy)=0.048)

计算信息增益
信息增益：系统的熵值从原始的0.940下降到了0.693，增益为0.247。

同样的方式可以计算出其他特征的信息增益，那么我们选择最大的那个，相当于是遍历了一遍特征，找出来了大当家，然后再其余中继续通过信息增益找二当家!

（找：信息增益大，熵值小）

五、信息增益率与gini系数
决策树算法
ID3
信息增益(有什么问题呢?)

问题：ID当做特征，熵值为0，不适合解决稀疏特征，种类非常多的。

C4.5
信息增益率(解决ID3问题，考虑自身熵)

CART
使用GINI系数来当做衡量标准

GINI系数
\large Gini(p)=\sum_{k=1}^{K}p_{k}(1-p_{k})=1-\sum_{k=1}^{K}p_{k}^{2}

(和熵的衡量标准类似，计算方式不相同)

连续值
进行离散化。



六、预剪枝方法
决策树剪枝策略
为什么要剪枝
决策树过拟合风险很大，理论上可以完全分得开数据 (想象一下，如果树足够庞大，每个叶子节点不就一个数据了嘛)

剪枝策略
（预剪枝，后减枝）

预剪枝
边建立决策树过程中进行剪枝的操作(更实用)。

限制深度，叶子节点个数。叶子节点样本数，信息增益量等。

七、后剪枝方法
后剪枝：当建立完决策树后来进行剪枝操作。

通过一定的衡量标准\large C_{a}(T)=C(T)+\alpha \left | T_{leaf} \right |

 \large C_{a}(T)：损失

\large C(T)：gini系数

\large T_{leaf}：叶子节点个数

(叶子节点越多，损失越大)



八、回归问题解决
回归问题将方差作为衡量（评估）标准。看标签的平均方差。

分类问题将熵值作为衡量标准。
```
